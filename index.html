<!DOCTYPE html>
<!-- saved from url=(0030)http://web.cs.ucla.edu/~zyuan/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Hanzhe Huâ€˜s Homepage">
    <meta name="author" content="Hanzhe Hu">
    <!--link rel="icon" href="./files/nyu_icon.ico"-->

    <title>Hanzhe Hu</title>

    <!-- Bootstrap core CSS -->
    <link href="./files/bootstrap.min.css" rel="stylesheet">
    <link href="./files/hzhu-homepage.css" rel="stylesheet">
</head>

<body>
<!--
__        __   _
\ \      / /__| | ___ ___  _ __ ___   ___
 \ \ /\ / / _ \ |/ __/ _ \| '_ ` _ \ / _ \
  \ V  V /  __/ | (_| (_) | | | | | |  __/
   \_/\_/ \___|_|\___\___/|_| |_| |_|\___|
-->

<!-- Hey there!! -->
<!-- If you see this message, you are probably considering building your own homepage, aren't you? -->
<!-- You are WELCOMED to use the template of this website. However, please read the following before you do so: -->
<!-- 1. You must agree the Creative Commons license listed at the bottom of this page, and follow the -->
<!--    instructions of the license; you need to acknowledge and distribute your website under the same license; -->
<!-- 2. Please REMOVE the website tracking code below or REPLACE them with your own; -->
<!--    You do not want me to know who visited your homepage, do you? ;) -->

<!-- I am so INSPIRED to see many academic researchers are now adopting this template which I created in 2015. -->
<!-- Here is an incomplete list that I know of: -->

<!--
    <ul>
        <li>http://web.cs.ucla.edu/~yuanjie.li/ </li>
        <li>http://vast.cs.ucla.edu/~peipei/ </li>
        <li>http://vast.cs.ucla.edu/~cody/ </li>
        <li>http://www.cs.ucr.edu/~kkhas001/ </li>
        <li>http://www.cs.ucr.edu/~dtrip003/ </li>
    </ul>
-->

<!-- I am proud and glad that you guys like my homepage template!!! 2f4650-->
<!-- Therefore, I decide to make this template open source, so more people can benefit from using this template. -->
<!-- Please contribute your ideas, raise issues, or commit codes at https://github.com/zwyuan/phd-homepage-template -->

<!-- Fixed navbar -->
<nav class="navbar navbar-inverse navbar-fixed-top" style="background-color: #8f1814">
    <div class="container" style="background-color: #8f1814">
        <div class="navbar-header ">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://hzhupku.github.io/"><strong>Hanzhe Hu</strong></a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav">
                <li class="active">
                    <a href="https://hzhupku.github.io/">Home</a>
                </li>
                <li>
                    <a href="https://hzhupku.github.io/#about">About</a>
                </li>
                <li>
                    <a href="https://hzhupku.github.io/#publications">Publications</a>
                </li>
                <li>
                    <a href="https://hzhupku.github.io/#research">Research</a>
                </li>
                <li>
                    <a target="_blank" href="./files/CV_HanzheHu_PKU.pdf">CV</a>
                </li>
                
            </ul>
        </div><!--/.nav-collapse -->
    </div>
</nav><!-- Fixed navbar-collapse -->

<!-- Begin page content -->
<div class="container">
    <!--Body content-->
    <div class="starter-template">

        <div class="row row-offcanvas row-offcanvas-right">

            <div class="col-xs-6 col-sm-3"><img alt="Nanjing, 2019" src="./files/hhz.JPG" height="225px" width="245" class="img-rounded"><br>
            <!--a target="_blank" href="./files/sqwang-taipei.jpg"--></a>
            </div>

            <div class="col-xs-12 col-sm-8">
                <h1>
                    <strong>Hanzhe Hu</strong><br>
                    <small>School of EECS, Peking University</small>
                </h1>
                <p>
                    No.5 Yiheyuan Road<br>
                    <!--5th Floor, Office 506<br>-->
                    Peking University<br>
                    Beijing, 100871, P.R.China
                </p>

                <p>Email: <code>huhz[at]pku.edu.cn</code></p>
            </div>
            <!-- <div class="col-xs-2 col-sm-1"></div> -->
            <!-- <br> -->
        </div>


        <hr size="1" color="#800000">
        <div id="about">
            <p style="text-align:justify; max-width:97%">I'm a second-year Computer Science Master student at School of EECS 
            of <a target="_blank" href="http://english.pku.edu.cn/">Peking University</a>. 
            Here I am working with Prof.<a target="_blank" href="https://scholar.google.com/citations?user=mgOosuUAAAAJ&hl=en&oi=ao">Jinshi Cui</a> and Prof.<a target="_blank" href="http://www.liweiwang-pku.com/">Liwei Wang</a>.
            Prior to PKU, I got my Bachelor's degree in Physics from <a target="_blank" href="http://www.nju.edu.cn/">Nanjing University</a>
            and worked at <a target="_blank" href="http://www.lamda.nju.edu.cn/MainPage.ashx">LAMDA</a> of NJU led by Prof.<a target="_blank" href="https://cs.nju.edu.cn/zhouzh/">Zhihua Zhou</a>.</p>

            <p style="text-align:justify; max-width:97%">My research interests are Computer Vision, Machine Learning and Reinforment Learning. Currently, I am focusing on few-shot detection & segmentation.
            Here is my <a target="_blank" href="./files/CV_HanzheHu_PKU.pdf">resume</a>.
            You can also view my <a href="https://scholar.google.com/citations?user=LjTCVxAAAAAJ&hl=zh-CN">Google Scholar profile</a>. I am looking for Ph.D position.</p>
        </div>
        <!--<h1 class="text-hide"><a id="about">Custom heading</a></h1>-->


        <hr size="2" color="#800000">
        <h2><b><a id="news">What's New</a></b></h2>

        <!-- Stack the columns on mobile by making one full-width and the other half-width -->
        <div class="row">
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;03/01/2021</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by CVPR2021!</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;1/18/2021</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is submitted to IJCAI2021.</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;12/02/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by AAAI2021!</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;11/18/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>Three papers are submitted to CVPR2021.</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;07/03/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by ECCV2020!</p></div>   
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;06/21/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by ICPR2020.</p></div>   
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;03/05/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>Two papers are submitted to ECCV2020.</p></div>    
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;02/01/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by CVPR2020!</p></div>
    
        </div>
      

        
        <!-- <dl class="dl-horizontal"> -->
            <!-- <ul style="list-style:none; margin:0px; padding:0px;"> --> 
            <!-- <dt>09 / 2019</dt> -->
            <!-- <dd>My homepage is built today!</dd>                -->
            <!-- </ul> -->

            <div align="right">
                <button type="button" class="btn btn-success btn-xs" data-target="#old-news" data-toggle="collapse">Older news</button>
            </div>
        </dl>
        <div id="old-news" class="collapse">
                <dl>
            </dl>
        </div>


        <hr size="1" color="#800000">
        <h2><b><a id="publications">Publications</a></b></h2>
        <!--<p>Other lists: <a href="http://scholar.google.com/citations?user=u42HjrwAAAAJ">Google Scholar</a>; <a href="http://dblp.uni-trier.de/pers/hd/y/Yuan:Zengwen">DBLP</a></p>-->


        <h3>Conference Paper</h3>

        <div class="panel panel-default">
            <table class="table table-hover">
                <tbody>
                <!--<tr>
                    <td>
                        <span class="label label-primary">C5</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Meta-Seg: Towards General Few-shot Segmentation
                            <span class="badge"><a class="conf" target="_blank" href="http://cvpr2021.thecvf.com/">CVPR '21</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>, Yiru Wang, Weihao Gan, Deyi Ji, Wei Wu, Junjie Yan, Jinshi Cui, Liwei Wang<br>
                            Under Review.<br>
                            
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#cvpr21-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#cvpr21-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/cvpr21.pdf">PDF</a></button>
                        <p></p>
                        <div id="cvpr21-abs" class="collapse">

                        </div>
                        <div id="cvpr21-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>-->
                <tr>
                    <td>
                        <span class="label label-primary">C5</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection
                            <span class="badge"><a class="conf" target="_blank" href="http://cvpr2021.thecvf.com/">CVPR '21</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>,  Shuai Bai, Aoxue Li, Jinshi Cui, Liwei Wang<br>
                            Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.<br>
                            
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#cvpr21-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#cvpr21-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/cvpr21.pdf">PDF</a></button>
                        <p></p>
                        <div id="cvpr21-abs" class="collapse">
                            Conventional deep learning based methods for object detection require a large amount of bounding box annotations for training, which is expensive to obtain such high quality annotated data. Few-shot object detection, which learns to adapt to novel classes with only a few annotated examples, is very challenging since the fine-grained feature of novel object can be easily overlooked with only a few data available. In this work, aiming to fully exploit features of annotated novel object and capture fine-grained features of query object, we propose Dense Relation Distillation with Context-aware Aggregation (DCNet) to tackle the few-shot detection problem. Built on the meta-learning based framework, Dense Relation Distillation module targets at fully exploiting support features, where support features and query feature are densely matched, covering all spatial locations in a feed-forward fashion. The abundant usage of the guidance information endows model the capability to handle common challenges such as appearance changes and occlusions. Moreover, to better capture scale-aware features, Context-aware Aggregation module adaptively harnesses features from different scales for a more comprehensive feature representation. Extensive experiments illustrate that our proposed approach achieves state-of-the-art results on PASCAL VOC and MS COCO datasets. Code will be made available at https://github.com/hzhupku/DCNet.
                        </div>
                        <div id="cvpr21-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>
                <tr>
                    <td>
                        <span class="label label-primary">C4</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Context-aware Graph Convolution Network for Target Re-identification
                            <span class="badge"><a class="conf" target="_blank" href="https://aaai.org/Conferences/AAAI-21/">AAAI '21</a></span>
                        </h4>
                        <p class="author">
                            Deyi Ji, Haoran Wang, <strong>Hanzhe Hu</strong>,  Weihao Gan, Wei Wu, Junjie Yan<br>
                            In Proceedings of AAAI Conference on Aritifical Intelligence.<br>
                            
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#aaai21-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#aaai21-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/aaai21.pdf">PDF</a></button>
                        <p></p>
                        <div id="aaai21-abs" class="collapse">
                                Most existing re-identification methods focus on learning robust and discriminative features with deep convolution networks. However, many of them consider content similarity separately and fail to utilize the context information of the query and gallery sets, e.g. probe-gallery and gallery-gallery relations, thus hard samples may not be well solved due to the limited or even misleading information. In this paper, we present a novel Context-Aware Graph Convolution Network (CAGCN), where the probe-gallery relations are encoded into the graph nodes and the graph edge connections are well controlled by the gallery-gallery relations. In this way, hard samples can be addressed with the context information flows among other easy samples during the graph reasoning. Specifically, we adopt an effective hard gallery sampler to obtain high recall for positive samples while keeping a reasonable graph size, which can also weaken the imbalanced problem in training process with low computation complexity. Experiments show that the proposed method achieves state-of-the-art performance on both person and vehicle re-identification datasets in a plug and play fashion with limited overhead.
                        </div>
                        <div id="aaai21-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>
                <tr>
                    <td>
                        <span class="label label-primary">C3</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Class-wise Dynamic Graph Convolution for Semantic Segmentation
                            <span class="badge"><a class="conf" target="_blank" href="https://eccv2020.eu/">ECCV '20</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>, Deyi Ji, Weihao Gan, Shuai Bai, Wei Wu, Junjie Yan<br>
                            In Proceedings of European Conference on Computer Vision.<br>
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#eccv20-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#eccv20-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/eccv20.pdf">PDF</a></button>
                        <p></p>
                        <div id="eccv20-abs" class="collapse">
                            Recent works have made great progress in semantic segmentation by exploiting contextual information in a local or global manner
                            with dilated convolutions, pyramid pooling or self-attention mechanism.
                            In order to avoid potential misleading contextual information aggregation in previous works, we propose a class-wise dynamic graph convolution(CDGC) module to adaptively propagate information. The graph
                            reasoning is performed among pixels in the same class. Based on the
                            proposed CDGC module, we further introduce the Class-wise Dynamic
                            Graph Convolution Network(CDGCNet), which consists of two main
                            parts including the CDGC module and a basic segmentation network,
                            forming a coarse-to-fine paradigm. Specifically, the CDGC module takes
                            the coarse segmentation result as class mask to extract node features
                            for graph construction and performs dynamic graph convolutions on the
                            constructed graph to learn the feature aggregation and weight allocation.
                            Then the refined feature and the original feature are fused to get the final
                            prediction. We conduct extensive experiments on three popular semantic segmentation benchmarks including Cityscapes, PASCAL VOC 2012
                            and COCO Stuff, and achieve state-of-the-art performance on all three
                            benchmarks.
                        </div>
                        <div id="eccv20-bib" class="collapse">
                            @misc{hu2020classwise,
                                title={Class-wise Dynamic Graph Convolution for Semantic Segmentation},
                                author={Hanzhe Hu and Deyi Ji and Weihao Gan and Shuai Bai and Wei Wu and Junjie Yan},
                                year={2020},
                                eprint={2007.09690},
                                archivePrefix={arXiv},
                                primaryClass={cs.CV}
                            }
                        </div>
                    </td>
                </tr>


                
                <tr>
                    <td>
                        <span class="label label-primary">C2</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Boundary-aware Graph Convolution for Semantic Segmentation
                            <span class="badge"><a class="conf" target="_blank" href="https://www.micc.unifi.it/icpr2020/">ICPR '20</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>, Jinshi Cui, Hongbin Zha<br>
                            International Conference on Pattern Recognition.<br>
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#icpr20-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#icpr20-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/icpr20.pdf">PDF</a></button>
                        <p></p>
                        <div id="icpr20-abs" class="collapse">
                                Recent works have made great progress in semantic segmentation by exploiting contextual information in a local or global manner with dilated convolutions, pyramid pooling or self-attention mechanism. However, few works have focused on harvesting boundary information to improve the segmentation performance. In order to enhance the feature similarity within the object and keep discrimination from other objects, we propose a boundary-aware graph convolution (BGC) module to propagate features within the object. The graph reasoning is performed among pixels of the same object apart from the boundary pixels. Based on the proposed BGC module, we further introduce the Boundary-aware Graph Convolution Net- work(BGCNet), which consists of two main components including a basic segmentation network and the BGC module, forming a coarse-to-fine paradigm. Specifically, the BGC module takes the coarse segmentation feature map as node features and boundary prediction to guide graph construction. After graph convolution, the reasoned feature and the input feature are fused together to get the refined feature, producing the refined segmentation result. We conduct extensive experiments on three popular semantic segmentation benchmarks including Cityscapes, PASCAL VOC 2012 and COCO Stuff, and achieve state-of-the-art performance on all three benchmarks.
                        </div>
                        <div id="icpr20-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>          

                    <tr>     
                    <td>
                        <span class="label label-primary">C1</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Adaptive Dilated Network with Self-Correction Supervision for Counting
                            <span class="badge"><a class="conf" target="_blank" href="http://cvpr2020.thecvf.com/">CVPR '20</a></span>
                        </h4>
                        <p class="author">
                            Shuai Bai, Zhiqun He, Yu Qiao, <strong>Hanzhe Hu</strong>, Wei Wu, Junjie Yan<br>
                            Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.<br>
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#cvpr20-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#cvpr20-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/cvpr20.pdf">PDF</a></button>
                        <p></p>
                        <div id="cvpr20-abs" class="collapse">
                            The counting problem aims to estimate the number of ob- jects in images. Due to large scale variation and labeling deviations, it remains a challenging task. The static den- sity map supervised learning framework is widely used in existing methods, which uses the Gaussian kernel to gen- erate a density map as the learning target and utilizes the Euclidean distance to optimize the model. However, the framework is intolerable to the labeling deviations and can not reflect the scale variation. In this paper, we propose an adaptive dilated convolution and a novel supervised learn- ing framework named self-correction (SC) supervision. In the supervision level, the SC supervision utilizes the out- puts of the model to iteratively correct the annotations and employs the SC loss to simultaneously optimize the model from both the whole and the individuals. In the feature level, the proposed adaptive dilated convolution predicts a continuous value as the specific dilation rate for each loca- tion, which adapts the scale variation better than a discrete and static dilation rate. Extensive experiments illustrate that our approach has achieved a consistent improvement on four challenging benchmarks. Especially, our approach achieves better performance than the state-of-the-art meth- ods on all benchmark datasets.
                        </div>
                        <div id="cvpr20-bib" class="collapse">
                            @InProceedings{Bai_2020_CVPR,
                                author = {Bai, Shuai and He, Zhiqun and Qiao, Yu and Hu, Hanzhe and Wu, Wei and Yan, Junjie},
                                title = {Adaptive Dilated Network With Self-Correction Supervision for Counting},
                                booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                                month = {June},
                                year = {2020}
                                }
                        </div>
                    </td>
                </tr>

                </tbody>
            </table>
        </div>

        <h3>Journal Article</h3>
       <!--
        <div class="panel panel-default">
            <table class="table table-hover">
                <tbody>
                    <tr>
                        <td>
                            <span class="label label-primary">J1</span>
                        </td>
                        <td>
                            <h4 class="list-group-item-heading">
                                A New Grid Structure for Adaptive and Anisotropic Fluid Simulation
                               
                            </h4>
                            <p class="author">
                                Anonymous author(s)<br>
                                ACM Transactions on Graphics (TOG), 2019. (Not submitted)<br>
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
    -->

        
        <h3>Thesis</h3>

        <div class="panel panel-default">
            <!--<div class="panel-body">-->
            <table class="table table-hover">
                <tbody>
                    <tr>
                        <td>
                            <span class="label label-primary">T1</span>
                        </td>
                        <td>
                            <h4 class="list-group-item-heading">
                                Deep Supervised Facial Expression Recognition.
                            </h4>
                            <p class="author">
                                <strong>Hanzhe Hu</strong><br>
                                Bachelor's thesis, Nanjing University, 2019.<br>
                            </p>                           
                            <button type="button" class="btn btn-success btn-xs" data-target="#bachelor-thesis-abs" data-toggle="collapse">Abstract</button>
                            <button type="button" class="btn btn-danger btn-xs"><a class="conf" target="_blank" href="./files/bachelor-slides.pdf">Defense Slides (Chinese)</a></button>
                            <div id="bachelor-thesis-abs" class="collapse">
                                    As the great success of machine learning and deep learning, face recognition has
                                    achieved unprecedented development with sufficient data to assist the learning process
                                    of deep learning algorithms. The current facial expression recognition algorithms mainly focus on two important problems: the over-fitting
                                    problem caused by insufficient amount of data and the recognition deviation caused by
                                    illumination, head pose and identity bias. This paper proposes solutions for facial expression recognition tasks in laboratorycontrolled scenes and in-the-wild scenes. First of all, in the laboratory-controlled scenario, due to the lack of data, the use of deep neural networks such as resnet-101 is likely
                                    to cause over-fitting. This paper utilizes the idea of ensemble learning, using multiple convolutional neural networks as sub-classifiers and outputs the final result. This
                                    method achieves the best recognition accuracy on both the FER2013 and CK+ datasets.
                                    Secondly, in the natural scenes, this paper focuses on solving the recognition problem
                                    caused by the invariance of head poses. In former research, the use of the generative
                                    adversarial network (GAN) for this task has been put forward, but the algorithm normally ignores the expression information. In this paper, the updated encoder-decoder
                                    mechanism is used to encode the expression information into the generator, and use it
                                    to generate a complete face with expression information for the expression recognition task. This method is tested on Multi-PIE dataset and achieves very impressive results.
                            </div>
                            <p></p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>

        <hr size="1" color="#800000">
        <h2><b><a id="research">Research</a></b></h2>

                <!-- <li class="list-group-item"> -->
                        <div class="panel panel-default">
                                <!-- <div class="panel-body"> -->
                                    <table class="table table-hover">
                                        <tbody>
                                            <!--
                                            <tr>
                                                <td>
                                                    <p></p>
                                                    <div class="row vert-offset-top-1 vert-offset-bottom-1">
                                                        <div class="col-md-4">
                                                            <img src="./files/cdgc2.png" class="img-thumbnail center-block" style="max-width:90%">
                                                        </div>
                                                        <div class="col-md-75">
                                                            <p><b>Class-wise Dynamic Graph Convolution for Semantic Segmentation</b><br>
                                                            <small>Peking University, SenseTime Group, advised by <a href="https://wuwei-ai.org/" target="_blank">Wei Wu</a></small> 
                                                            <p style="text-align:justify; max-width:97%"><small>Recent work has made great progress in semantic segmentation by exploit ing contextual information in a local or
                                                                    global manner with dilated convolutions, pyramid pooling or self-attention mechanism. In order to avoid problematic contextual information aggregation in previous work, we proposed a class-wise dynamic graph convolution(CDGC) module to adaptively propagate information. Our proposed framework achieves the state-of-art performance on three popular benchmark datasets: Cityscapes, Pascal Voc2012 and COCO Stuff. Our paper is under review in ECCV2020.</small></p>
                                                        </div>
                                                    </div>
                                                </td>
                                            </tr>
                                            -->
        

                                            <tr>
                                                <td>
                                                    <p></p>
                                                    <div class="row vert-offset-top-1 vert-offset-bottom-1">
                                                        <div class="col-md-4">
                                                            <img src="./files/lane2.png" class="img-thumbnail center-block" style="max-width:90%">
                                                        </div>
                                                        <div class="col-md-75">
                                                            <p><b>Lane-line semantic and instance segmentation</b><br>
                                                            <small>SenseTime Group, advised by <a href="https://wuwei-ai.org/" target="_blank">Wei Wu</a></small> 
                                                            <p style="text-align:justify; max-width:97%"><small>In smart city project, lane-line detection is important for anomaly detection such as breaking traffic rules. Currently,
                                                                    we implement the state-of-art semantic segmentation network including Deeplab v3+ and PSPNet, and perform Meanshift clustering algorithm as a post-processing method to obtain the instance segmentation result. Moreover, our method achieves the state-of-art result on Tusimple lane detection benchmark dataset.</small></p>
                                                        </div>
                                                    </div>
                                                </td>
                                            </tr>
                                            <tr>
                                                    <td>
                                                        <p></p>
                                                        <div class="row vert-offset-top-1 vert-offset-bottom-1">
                                                            <div class="col-md-4">
                                                                <img src="./files/zoopt2.png" class="img-thumbnail center-block" style="max-width: 90%" >
                                                            </div>
                                                            <div class="col-md-75">
                                                                <p><b>Derivative-free Optimization(ZOOpt, zero-th order optimization)</b><br>
                                                                <small>Nanjing University, advised by <a href="http://www.lamda.nju.edu.cn/yuy/" target="_blank">Prof. Yang Yu</a>  
                                                                </small><br>
                                                                <p style="text-align:justify; max-width:97%"><small>Our research mainly focused on accomplishing the python packages:ZOOpt and ZOOsrv(a distributed version of ZOOpt),
                                                                        and designing comparative experiments with other packages used for optimization including CMA-ES and BayesOpt.
                                                                        Comparative experiments are designed to compare efficiency of optimization on testing functions such as sphere Prize
                                                                        function and ackley function and on clustering and classification.</small></p>
                                                            </div>
                                                        </div>
                                                    </td>
                                            </tr>
                                        </tbody>
                                    </table>
                                <!-- </div> -->
                                </div>
        
                                <hr size="1" color="#800000">
        <h2><b><a id="awards">Experiences</a></b></h2>
        <div class="row">
            <div class="col-xs-2 col-md-2"><p align="center">
                <strong>
                    03/2021 - Present<br>
                    02/2019 - 03/2021<br>
                </strong></p>
            </div>
            <div class="col-xs-18 col-md-15" style="margin-right: 5px"><p>
                Research Intern at <a target="_blank" href="https://www.msra.cn/">MSRA (Microsoft Research Asia)</a><br>
                Research Intern at <a target="_blank" href="https://www.sensetime.com/en/">SenseTime Research</a>
            </p></div>
        </div>

        <hr size="1" color="#800000">
        <h2><b><a id="awards">Awards & Competitions</a></b></h2>
        <div class="row">
            <div class="col-xs-2 col-md-2"><p align="center">
                <strong>
                    10/2020<br>
                    01/2020<br>
                    09/2019<br>
                    06/2019<br>
                    09/2018<br>
                    10/2016<br>
                    09/2014<br>
                </strong></p>
            </div>
            <div class="col-xs-18 col-md-15" style="margin-right: 5px"><p>
                Scholarship for Outstanding Research (top 5%)<br>
                1st National AI Challenge(Re-ID) Rank 20/2100<br>
                Peking University Scholarship<br>
                Outstanding Graduates of Nanjing University<br>
                Scholarship of Xingquan Fund<br>
                Second-class Scholarship for Outstanding Student <br>
                Second-Prize in High School Students Physics & Mathematics Contest in China
            </p></div>
        </div>



        <hr size="1" color="#800000">
        <h2><b><a name="misc">Misc</a></b></h2>

        <!-- <p>I love photography and traveling. I own a Nikon D750 body. My current lens collection includes a Rokinon 14mm f/2.8, a Sigma 35mm f/1.4 Art, a Nikon 50mm f/1.4 D, a Nikon 85mm f/1.8 D, a Tokina AT-X 100mm f/2.8 macro, and a Nikon 18-35mm f/3.5-4.5 zoom. Read <a target="_blank" href="http://zwyuan.github.io/2017/12/31/my-memories-of-the-year-2017-in-photos/">my pick of photos in 2017</a> or see more of the photos I took at <a target="_blank" href="https://500px.com/zwyuan">my 500px photo gallery</a>, and images available for licensing on <a href="https://www.gettyimages.com/search/photographer?family=creative&amp;photographer=edward%20yuan&amp;sort=best">Getty Images</a>.</p> -->
        <p>An INTJ. I love traveling and reading books. Also, I play StarCraft II in my spare time.  </p>

        <p id="phd-day-count">It's been 1470 days since I started !<br>

        <script>
        var montharray=new Array("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")

        function countup(yr,m,d){
        var today=new Date()
        var todayy=today.getYear()
        if (todayy < 1000)
        todayy+=1900
        var todaym=today.getMonth()
        var todayd=today.getDate()
        var todaystring=montharray[todaym]+" "+todayd+", "+todayy
        var paststring=montharray[m-1]+" "+d+", "+yr
        var difference="It's been " + (Math.round((Date.parse(todaystring)-Date.parse(paststring))/(24*60*60*1000))*1) + " days since I joined Peking University.<br>"
            document.getElementById('phd-day-count').value = difference
            document.getElementById('phd-day-count').innerHTML = difference
        }
        //enter the count up date using the format year/month/day
        countup(2019,09,03)
        </script>
        <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5zvnamk8exm&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
        <!-- Oh, and please REMOVE or REPLACE this counter as well :) -->
        <!-- <p> -->
        <!--
            You are the No. <a href="https://www.easycounter.com/">
                <img src="https://www.easycounter.com/counter.php?hanzhe97"
                border="0" alt="HTML Hit Counters"></a> th vistor of my homepage.
        -->
        </p>
    </div>
</div><!--/.container-->

<footer class="footer">
    <div class="container" style="background-color: #ebebeb">
        <p></p>
        <p class="text-muted" align="center" style="color: rgb(51,51,51); line-height:18pt">
            Copyright &#169 Hanzhe Hu <script>var d = new Date(); document.write(d.getFullYear())</script>
        </p>
        <p class="text-muted" align="center" style="color: rgb(51,51,51); line-height:12pt">
            <small><small>
                <script>
                    var lastModTime = new Date(document.lastModified);
                    document.write("Last Modified: "+lastModTime)
                </script><br>
                <!--This work is licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International</a> License and modified from <a href="http://web.cs.ucla.edu/~zyuan/">Zengwen Yuan's Homepage</a>.-->                
            </small></small>   
        </p>

        <!-- PLEASE KEEP THIS LICENSE -->
        <!-- Your use of the template of this website means that you agree to this license. -->
       
    </div>
</footer>

<!-- Bootstrap core JavaScript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<!--<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>-->
<script src="./files/jquery-2.1.4.min.js"></script>
<script src="./files/bootstrap.min.js"></script>


</body></html>
